# ETL Pipeline Last.fm
![Схема проекта](./pictures/ETL_Pipeline.jpg)

## Цель и задачи проекта
Цель - построить ETL Pipeline для хранения и анализа данных, получаемых из API источника.

Задачи:
1. Настроить извлечение сырых данных из API источника.
2. Построить ETL процесс для загрузки данных в хранилище.
3. Построить витрины данных для дальнейшей визуализации и анализа в BI системе.

# Описание проекта
## Используемые инструменты
- **Docker** - создание контейнеров для основных компонентов
- **Airflow** - оркестрация процессов
- **Python** - извлечение и обработка данных
- **Postgres** - хранилище данных
- **Minio S3** - озеро данных
- **Metabase** - визуализация и анализ данных

## Извлечение данных из API
В качестве источника данных используется открытое API Last.fm, а именно метод "geo.getTopTracks", который возвращает топ треков на текущий момент времени по заданной стране. В данном проекте, в качестве примера, используются 3 страны: Russian Federation, United States, Kazakhstan. Запрос отправляется при помощи библиотеки requests языка Python.

По запросу из API извлекается топ 100 треков для каждой из заданных стран в формате JSON. Полученные файлы в необработанном виде помещаются в озеро данных (S3 хранилище) на холодное хранение. Описанный процесс запускается ежедневно при помощи оркестратора Airflow (DAG - "raw_from_api_to_s3.py").

## Построение ETL процесса 
Для построения ETL процесса применяется язык Python, так как получаемый объем данных небольшой и параллельная обработка не требуется. 
В качестве хранилища данных используется Postgres, состоящий из 3-х слоев: 
* ODS - таблица для хранения прошедших через ETL процесс данных;
* DDS - данные хранятся в виде модели "звезда", разбитые на таблицы фактов и измерений;
* DM - слой для витрин данных.

Извлечение, трансформация и загрузка данных выполняется при помощи DAG'a "transformed_from_s3_to_pg.py". В нем применяется ExternalTaskSensor, который ожидает завершения выполнения DAG'a с предыдущего шага, а затем начинает работу запланированных task'ов. Первым из них извлекаются ключи для хранящихся за текущую дату файлов из S3 хранилища. Полученные ключи передаются в следующий task через xcom. Загруженные по ключам JSON данные проходят через трансформацию: из файлов извлекается необходимая информация, происходит очистка и преобразование типов данных. Трансформированные данные загружаются в Postgres базу данных на ODS слой при помощи SQL скрипта.

Следующим шагом данные из ODS слоя передаются в DDS слой в виде "звезды" для дальнейшей аналитики. За это отвечает DAG "from_ods_to_dds_pg.py". В нем, для передачи данных с помощью SQL скриптов, применяется SQLExecuteQueryOperator.

## Создание витрин данных для BI системы
Для примера были созданы две витрины данных: "avg_song_duration_by_country" и "artist_appearances_by_date". Для заполнения витрин данных используется DAG "from_dds_to_dm.py", работающий аналогичным образом с DAG'ом с предыдущего шага.

В качестве BI системы используется Metabase, в котором строятся графики и дашборды по имеющимся витринам данных.